{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import torch\n",
    "import pandas as pd\n",
    "import evaluate\n",
    "from datasets import Dataset\n",
    "from tqdm.auto import tqdm\n",
    "from torch.optim import AdamW\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, get_scheduler\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, ConfusionMatrixDisplay\n",
    "\n",
    "\n",
    "############################################################################################################\n",
    "# model name\n",
    "# choose from huggingface\n",
    "model_name = \"cl-tohoku/bert-base-japanese-v3\"\n",
    "# マージ元\n",
    "from_emoji = [\"😂\", \"💕\", \"☺️\", \"😃\", \"😆\", \"😁\", \"🥲\", \"👍\", \"✨\", \"☀️\", \"😅\", \"🥺\"]\n",
    "# マージ先\n",
    "to_emoji = [\"🤣\", \"🥰\", \"😊\", \"😊\", \"😊\", \"😊\", \"😭\", \"😊\", \"😊\", \"😊\", \"🤣\", \"😭\"]\n",
    "# ラベル\n",
    "label2id_temp = {\"😊\": 0, \"🤣\": 1, \"😭\": 2, \"🎉\": 3, \"🥰\": 4, \"😇\": 5, \"💦\": 6, \"🤔\": 7}\n",
    "############################################################################################################\n",
    "\n",
    "# label2id\n",
    "label2id = {}\n",
    "cnt = 0\n",
    "for e, i in label2id_temp.items():\n",
    "    label2id[e] = cnt\n",
    "    cnt = cnt + 1\n",
    "# ラベル\n",
    "label = [e for e in label2id]\n",
    "# ラベル数\n",
    "num_labels = len(label)\n",
    "# id2label\n",
    "id2label = {}\n",
    "for e, i in label2id.items():\n",
    "    id2label[i] = e\n",
    "\n",
    "\n",
    "# json ファイルから pandas に読み込む\n",
    "def load_json_to_pandas(json_path):\n",
    "\n",
    "    data = pd.read_json(json_path)\n",
    "\n",
    "    # 絵文字マージ\n",
    "    if from_emoji:\n",
    "        for index, row in data.iterrows():\n",
    "            if row[\"label\"] in from_emoji:\n",
    "                data.at[index, \"label\"] = to_emoji[from_emoji.index(row[\"label\"])]\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "# pandas から Dataset オブジェクト化する\n",
    "def pandas_to_Dataset(dataset):\n",
    "\n",
    "    dataset = Dataset.from_pandas(dataset)\n",
    "\n",
    "    return dataset\n",
    "\n",
    "\n",
    "# DataFrame 型のデータセットのラベルを整数に変える\n",
    "def label_to_int(dataset):\n",
    "\n",
    "    for index, row in dataset.iterrows():\n",
    "        dataset.at[index, \"label\"] = label2id[row[\"label\"]]\n",
    "\n",
    "    return dataset\n",
    "\n",
    "\n",
    "# トークン化\n",
    "def tokenize(data):\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "    return tokenizer(data[\"text\"], padding=\"max_length\", truncation=True, max_length=128)\n",
    "\n",
    "\n",
    "def main():\n",
    "\n",
    "    ############################################################################################################\n",
    "    # パラメータ設定\n",
    "    # 訓練データパス\n",
    "    train_data_path = \"./dataset/train_cls/R8.json\"\n",
    "    # テストデータのパス\n",
    "    test_data_path = \"./dataset/test/T8h.json\"\n",
    "    # 訓練データ数 \n",
    "    num_of_train_data = 313770\n",
    "    # テストデータ数\n",
    "    num_of_test_data = 360\n",
    "    # バッチサイズ\n",
    "    batch_size = 16\n",
    "    # エポック数\n",
    "    num_epoch = 2\n",
    "    # 学習率\n",
    "    learning_rate = 1e-5\n",
    "    ############################################################################################################\n",
    "    \n",
    "    # 生データの読み込み（絵文字マージ含む）\n",
    "    train_data = load_json_to_pandas(train_data_path)\n",
    "    test_data = load_json_to_pandas(test_data_path)\n",
    "\n",
    "    # ラベルを整数に変える\n",
    "    train_data = label_to_int(train_data)\n",
    "    test_data = label_to_int(test_data)\n",
    "\n",
    "    # Dataset オブジェクトに変える\n",
    "    train_data = pandas_to_Dataset(train_data)\n",
    "    test_data = pandas_to_Dataset(test_data)\n",
    "    #print(train_data[1])\n",
    "\n",
    "    # トークン化\n",
    "    train_data = train_data.map(tokenize, batched=True)\n",
    "    test_data = test_data.map(tokenize, batched=True)\n",
    "    #print(train_data[0])\n",
    "\n",
    "    # 入力に必要のない欄を削除\n",
    "    train_data = train_data.remove_columns([\"text\"])\n",
    "    test_data = test_data.remove_columns([\"text\"])\n",
    "    #print(train_data[0])\n",
    "\n",
    "    # label を labels に再命名\n",
    "    train_data = train_data.rename_column(\"label\", \"labels\")\n",
    "    test_data = test_data.rename_column(\"label\", \"labels\")\n",
    "\n",
    "    # pytorch tensors を返す\n",
    "    train_data.set_format(\"torch\")\n",
    "    test_data.set_format(\"torch\")\n",
    "    \n",
    "    # より小さいデータセットを作成する\n",
    "    train_data = train_data.shuffle(seed=42).select(range(num_of_train_data))\n",
    "    test_data = test_data.shuffle(seed=42).select(range(num_of_test_data))\n",
    "    #print(len(train_data), len(test_data))\n",
    "    #print(train_data[0])\n",
    "    #print(test_data[0])\n",
    "\n",
    "    # dataloader を作る\n",
    "    train_dataloader = DataLoader(train_data, shuffle=False, batch_size=batch_size)\n",
    "    eval_dataloader = DataLoader(test_data, batch_size=batch_size)\n",
    "\n",
    "    # モデルをロード\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=num_labels)\n",
    "\n",
    "    # 最適化関数、学習率の設定\n",
    "    optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # learning rate scheduler の作成\n",
    "    num_training_steps = num_epoch * len(train_dataloader)\n",
    "    lr_scheduler = get_scheduler(\n",
    "        name=\"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=num_training_steps\n",
    "        )\n",
    "    \n",
    "    # GPU に送る\n",
    "    device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "    model.to(device)\n",
    "\n",
    "    # プログレスバーの作成\n",
    "    progress_bar = tqdm(range(num_training_steps))\n",
    "\n",
    "    # 訓練\n",
    "    #model.train()\n",
    "    for epoch in range(num_epoch):\n",
    "        for batch in train_dataloader:\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            outputs = model(**batch)\n",
    "            loss = outputs.loss\n",
    "            loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "            lr_scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "            progress_bar.update(1)\n",
    "\n",
    "    # 検証\n",
    "    all_predictions = []\n",
    "    all_refs = []\n",
    "\n",
    "    metric_acc = evaluate.load(\"accuracy\")\n",
    "    metric_f1 = evaluate.load(\"f1\")\n",
    "    metric_f1_each = evaluate.load(\"f1\")\n",
    "\n",
    "    #model.eval()\n",
    "    for batch in eval_dataloader:\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**batch)\n",
    "\n",
    "        logits = outputs.logits\n",
    "\n",
    "        predictions = torch.argmax(logits, dim=-1)\n",
    "        predictions_topk = torch.topk(\n",
    "            input=logits,\n",
    "            k=5,\n",
    "            dim=-1,\n",
    "            sorted=True\n",
    "        )\n",
    "\n",
    "        predictions_topk = predictions_topk[1].tolist()\n",
    "        for pred in predictions_topk:\n",
    "            all_predictions.append(pred)\n",
    "        labels_per_batch = batch[\"labels\"].tolist()\n",
    "        for l in labels_per_batch:\n",
    "            all_refs.append(l)\n",
    "\n",
    "        metric_acc.add_batch(predictions=predictions, references=batch[\"labels\"])\n",
    "        metric_f1.add_batch(predictions=predictions, references=batch[\"labels\"])\n",
    "        metric_f1_each.add_batch(predictions=predictions, references=batch[\"labels\"])\n",
    "\n",
    "    result_acc = metric_acc.compute()\n",
    "    result_f1 = metric_f1.compute(average=\"macro\")\n",
    "    result_f1_each = metric_f1_each.compute(average=None)\n",
    "    print(result_acc)\n",
    "    print(result_f1)\n",
    "    print(result_f1_each)\n",
    "\n",
    "    num_true = []\n",
    "    for i in range(0, len(all_predictions), 1):\n",
    "        if all_refs[i] in all_predictions[i]:\n",
    "            num_true.append(1)\n",
    "    print(f\"A@5: {len(num_true)} / {len(all_predictions)} = {len(num_true) / len(all_predictions)}\")\n",
    "\n",
    "    y_true = np.array([id2label[l] for l in all_refs])\n",
    "    y_pred = np.array([id2label[l[0]] for l in all_predictions])\n",
    "\n",
    "    print(classification_report(y_true=y_true, y_pred=y_pred, labels=label, zero_division=0.0))\n",
    "\n",
    "    ConfusionMatrixDisplay.from_predictions(y_true=y_true, y_pred=y_pred, labels=label)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "emoji_prediction",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
