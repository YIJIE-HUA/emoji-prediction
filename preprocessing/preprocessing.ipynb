{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import csv\n",
    "import json\n",
    "import emoji\n",
    "import neologdn\n",
    "import unicodedata\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "# 絵文字の出現回数をカウントし、降順にソートする\n",
    "def emoji_count_and_sort(dataset, sort=True, descending_order=True):\n",
    "\n",
    "    # 絵文字の出現回数をカウントする\n",
    "    emoji_count = {}\n",
    "    for d in dataset:\n",
    "        if d[\"label\"] not in emoji_count:\n",
    "            emoji_count[d[\"label\"]] = 1\n",
    "        else:\n",
    "            emoji_count[d[\"label\"]] = emoji_count[d[\"label\"]] + 1\n",
    "\n",
    "    # ソートする\n",
    "    if sort:\n",
    "        emoji_count = dict(sorted(emoji_count.items(), key=lambda x:x[1], reverse=descending_order))\n",
    "        return emoji_count\n",
    "    \n",
    "    return emoji_count\n",
    "\n",
    "\n",
    "# テキサスクリーニング\n",
    "def clean_text(dataset, del_num=True):\n",
    "    for i in range(0, len(dataset), 1):\n",
    "        dataset[i][\"text\"] = re.sub(r'@[a-zA-Z0-9_]+', \"\", dataset[i][\"text\"])\n",
    "        dataset[i][\"text\"] = re.sub(r'http[a-zA-Z0-9_:./]+', \"\", dataset[i][\"text\"])\n",
    "        #dataset[i][\"text\"] = re.sub(r'[a-zA-Z_]', \"\", dataset[i][\"text\"])\n",
    "        if del_num:\n",
    "            dataset[i][\"text\"] = re.sub(r'[0-9]', \"0\", dataset[i][\"text\"])\n",
    "        dataset[i][\"text\"] = neologdn.normalize(dataset[i][\"text\"])\n",
    "\n",
    "    # 空の文字列の削除\n",
    "    dataset = [d for d in dataset if d[\"text\"]]\n",
    "    \n",
    "    return(dataset)\n",
    "\n",
    "\n",
    "# 全生ツイートの取得関数\n",
    "def get_tweets():\n",
    "\n",
    "    # ツイートデータのパスを取得する\n",
    "    p = Path('.')\n",
    "    csv_paths = list(p.glob('./dataset/**/*.csv'))\n",
    "    \n",
    "    # 全 csv からツイートを読み込む\n",
    "    tweets = []\n",
    "    for csv_path in csv_paths:\n",
    "        with open(csv_path, 'r', encoding='utf-8') as f:\n",
    "            reader = csv.reader(f)\n",
    "            texts_list = [row for row in reader]\n",
    "        # 読み込んだ多次元リストを1次元リストに変換\n",
    "        for tl in texts_list:\n",
    "            for i in range(0, len(tl), 1):\n",
    "                tweets.append(tl[i])\n",
    "    print(f\"全生ツイート: {len(tweets)}\")\n",
    "    #print(tweets[test_index])\n",
    "\n",
    "    return tweets\n",
    "\n",
    "\n",
    "# 絵文字が一種類しかないツイートの取得\n",
    "def get_one_emoji_tweets(text_cleaning=True, del_num=True, emoji_position_end=True, emoji_version=15, text_len_min=0, text_len_max=500, create_json=False):\n",
    "\n",
    "    # 全生ツイートの取得\n",
    "    tweets = get_tweets()\n",
    "\n",
    "    # NFKC正規化\n",
    "    tweets = [unicodedata.normalize(\"NFKC\", t) for t in tweets]\n",
    "    #print(len(tweets))\n",
    "    #print(tweets[test_index])\n",
    "\n",
    "    # 絵文字が一種類しかないテキストの抽出\n",
    "    one_emoji_tweets = [t for t in tweets if emoji.emoji_count(t, unique=True) == 1]\n",
    "    print(f\"\\n\\\n",
    "絵文字が一種類しかない生ツイートの数: {len(one_emoji_tweets)}\\\n",
    " (全生ツイートにおける割合: {(len(one_emoji_tweets) / len(tweets)) * 100}%)\\\n",
    "    \")\n",
    "    #print(one_emoji_tweets[test_index])\n",
    "\n",
    "    # emoji_version 以下の絵文字付きのツイートを取得する\n",
    "    num_of_one_emoji_tweets = len(one_emoji_tweets)\n",
    "    one_emoji_tweets = [t for t in one_emoji_tweets if emoji.EMOJI_DATA[emoji.distinct_emoji_list(t)[0]][\"E\"] <= emoji_version]\n",
    "    print(f\"\\n\\\n",
    "Unicode Version {emoji_version:.1f} 以下の絵文字付きツイートの数： {len(one_emoji_tweets)}\\\n",
    " (絵文字が一種類しかないツイートにおける割合： {(len(one_emoji_tweets) / num_of_one_emoji_tweets) * 100})\\\n",
    "          \")\n",
    "    \n",
    "    # 絵文字が末尾にあるツイートを取得する\n",
    "    if emoji_position_end:\n",
    "        one_emoji_tweets = [t for t in one_emoji_tweets if emoji.emoji_list(t)[0][\"match_end\"] == len(t)]\n",
    "        print(f\"\\n\\\n",
    "絵文字が末尾にあるツイートの数: {len(one_emoji_tweets)}\\\n",
    " (絵文字が一種類しかないツイートにおける割合: {(len(one_emoji_tweets) / num_of_one_emoji_tweets) * 100}%)\\\n",
    "    \")\n",
    "        \n",
    "    # 絵文字が一種類しかないツイートのデータセットの作成\n",
    "    one_emoji_dataset = [\n",
    "        {\"text\": emoji.replace_emoji(t, replace=\"\"),\"label\": emoji.distinct_emoji_list(t)[0]} \n",
    "        for t in one_emoji_tweets\n",
    "    ]\n",
    "    #print(len(one_emoji_dataset))\n",
    "\n",
    "    # テキストクリーニング\n",
    "    if text_cleaning:\n",
    "        one_emoji_dataset = clean_text(one_emoji_dataset, del_num=del_num)\n",
    "\n",
    "    # テキストの長さを制限する\n",
    "    one_emoji_dataset = [d for d in one_emoji_dataset if (len(d[\"text\"]) >= text_len_min) and (len(d[\"text\"]) <= text_len_max)]\n",
    "    print(f\"\\n\\\n",
    "長さ制限: {text_len_min} ~ {text_len_max}\\n\\\n",
    "長さ制限後のツイートの数: {len(one_emoji_dataset)}\\\n",
    "(絵文字が一種類しかないツイートにおける割合: {(len(one_emoji_dataset) / num_of_one_emoji_tweets) * 100}%)\\\n",
    "          \")\n",
    "\n",
    "    # jsonに出力\n",
    "    if create_json:\n",
    "        json_path = \"./dataset/one_emoji_dataset.json\"\n",
    "        with open(json_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump(one_emoji_dataset, f, indent=4, ensure_ascii=False)\n",
    "\n",
    "    return one_emoji_dataset\n",
    "\n",
    "\n",
    "# 出現回数上位 n 個の絵文字のツイートを取得する\n",
    "def get_topn_one_emoji_tweets(topn=20, text_cleaning=True, del_num=True, qualified_diff=True, emoji_position_end=True, emoji_version=15, text_len_min=0, text_len_max=500, create_json=False):\n",
    "\n",
    "    # 絵文字が一種類しかないツイートの取得\n",
    "    one_emoji_dataset = get_one_emoji_tweets(text_cleaning=text_cleaning,\n",
    "                                             del_num=del_num,\n",
    "                                             emoji_position_end=emoji_position_end,\n",
    "                                             emoji_version=emoji_version,\n",
    "                                             text_len_min=text_len_min,\n",
    "                                             text_len_max=text_len_max,\n",
    "                                             create_json=create_json\n",
    "                                             )\n",
    "\n",
    "    # 絵文字出現回数の取得\n",
    "    emoji_count = emoji_count_and_sort(one_emoji_dataset)\n",
    "    print(f\"\\\n",
    "\\n全絵文字種類数（ステータスマージ前）: {len(emoji_count)}\\\n",
    "（世界中の全絵文字における割合：{(len(emoji_count) / 5025) * 100}）\\\n",
    "          \")\n",
    "\n",
    "    # 各絵文字の絵文字が一種類しかないツイートにおける割合の取得\n",
    "    emoji_count_temp = {}\n",
    "    for e, n in emoji_count.items():\n",
    "        emoji_count_temp[e] = [n, \"{:.2f}\".format((n / len(one_emoji_dataset)) * 100)]\n",
    "    emoji_count = emoji_count_temp\n",
    "    #print(len(one_emoji_dataset))\n",
    "    print(f\"全絵文字とその出現回数（ステータスマージ前）: {emoji_count}\")\n",
    "\n",
    "    # ステータス違いを無視する時\n",
    "    if not qualified_diff:\n",
    "\n",
    "        # ステータス違いがある絵文字の取得\n",
    "        diff_status_emojis_list = []\n",
    "        for e in emoji_count.keys():\n",
    "            diff_status_emojis = [ec for ec in emoji_count.keys() if emoji.EMOJI_DATA[e][\"en\"] == emoji.EMOJI_DATA[ec][\"en\"]]\n",
    "            if (diff_status_emojis not in diff_status_emojis_list) and (len(diff_status_emojis) != 1):\n",
    "                diff_status_emojis_list.append(diff_status_emojis)\n",
    "        print(f\"\\nステータス違いがある絵文字の種類数: {len(diff_status_emojis_list)}\")\n",
    "        print(f\"ステータス違いがある絵文字: {diff_status_emojis_list}\")\n",
    "\n",
    "        # fully-qualified じゃない絵文字を fully-qualified 絵文字に変える\n",
    "        for i in range(0, len(one_emoji_dataset), 1):\n",
    "            for el in diff_status_emojis_list:\n",
    "                if one_emoji_dataset[i][\"label\"] in el:\n",
    "                    for e in el:\n",
    "                        if emoji.EMOJI_DATA[e][\"status\"] == 2:\n",
    "                            one_emoji_dataset[i][\"label\"] = e\n",
    "        emoji_count = emoji_count_and_sort(one_emoji_dataset)\n",
    "        print(f\"\\\n",
    "\\n全絵文字種類数（ステータスマージ後）: {len(emoji_count)}\\\n",
    "（世界中の全絵文字における割合：{(len(emoji_count) / 5025) * 100}、fully-qualifiedの全絵文字における割合：{(len(emoji_count) / 3773) * 100}）\\\n",
    "              \")\n",
    "\n",
    "        # 各絵文字の絵文字が一種類しかないツイートにおける割合の取得\n",
    "        emoji_count_temp = {}\n",
    "        for e, n in emoji_count.items():\n",
    "            emoji_count_temp[e] = [n, \"{:.2f}\".format((n / len(one_emoji_dataset)) * 100)]\n",
    "        emoji_count = emoji_count_temp\n",
    "        #print(len(one_emoji_dataset))\n",
    "        print(f\"全絵文字とその出現回数（ステータスマージ後）: {emoji_count}\")\n",
    "\n",
    "    # トップ n 個の絵文字の取得\n",
    "    i = 0\n",
    "    emoji_count_topn_total = 0\n",
    "    emoji_count_topn_temp = {}\n",
    "    emoji_count_topn = {}\n",
    "    for e, ei in emoji_count.items():\n",
    "        if i >= topn:\n",
    "            break\n",
    "        emoji_count_topn_temp[e] = ei\n",
    "        emoji_count_topn_total = emoji_count_topn_total + ei[0]\n",
    "        i = i + 1\n",
    "    for e, ei in emoji_count_topn_temp.items():\n",
    "        ei[1] = (ei[0] / emoji_count_topn_total) * 100\n",
    "        ei[1] = \"{:.2f}\".format(ei[1])\n",
    "        emoji_count_topn[e] = ei\n",
    "    print(f\"\\n出現回数トップ {topn} 絵文字: {emoji_count_topn}\")\n",
    "\n",
    "    # トップ n 個の絵文字を含むデータセットの作成\n",
    "    topn_one_emoji_dataset = [d for d in one_emoji_dataset if d[\"label\"] in emoji_count_topn]\n",
    "    print(f\"出現回数トップ {topn} 絵文字を含むツイート数: {len(topn_one_emoji_dataset)}\")\n",
    "\n",
    "    # json に出力\n",
    "    if create_json:\n",
    "        json_path = f\"./dataset/top{topn}_one_emoji_dataset.json\"\n",
    "        with open(json_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump(topn_one_emoji_dataset, f, indent=4, ensure_ascii=False)\n",
    "\n",
    "\n",
    "def main():\n",
    "    ############################################################################################################\n",
    "    # パラメータ設定\n",
    "\n",
    "    # 出現回数上位何位まで取り出す\n",
    "    topn = 20\n",
    "    # fully-qualified 絵文字にマージするかどうか、False はマージする\n",
    "    qualified_diff = False\n",
    "    # テキストクリーニングを行うかどうか\n",
    "    text_cleaning = True\n",
    "    # 数字を削除するかどうか\n",
    "    del_num = False\n",
    "    # 絵文字が末尾にあるツイートを取得するかどうか\n",
    "    emoji_position_end = True\n",
    "    # emoji_version 以下の絵文字付きのツイートの取得\n",
    "    emoji_version = 13.1\n",
    "    # テキスト長さ制限\n",
    "    text_len_min = 0\n",
    "    text_len_max = 5000\n",
    "    # jsonファイルに出力するかどうか\n",
    "    create_json = True\n",
    "    ############################################################################################################\n",
    "    \n",
    "    get_topn_one_emoji_tweets(topn=topn,\n",
    "                              qualified_diff=qualified_diff,\n",
    "                              text_cleaning=text_cleaning,\n",
    "                              del_num=del_num,\n",
    "                              emoji_position_end=emoji_position_end,\n",
    "                              emoji_version=emoji_version,\n",
    "                              text_len_min=text_len_min,\n",
    "                              text_len_max=text_len_max,\n",
    "                              create_json=create_json\n",
    "                              )\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "emoji_prediction",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
