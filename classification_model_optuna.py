import numpy as np
import pandas as pd
import evaluate
from datasets import Dataset
from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer
from sklearn.metrics import classification_report
from sklearn.model_selection import train_test_split


############################################################################################################
# ä½¿ã†ãƒ¢ãƒ‡ãƒ«
model_name = "FacebookAI/xlm-mlm-17-1280"

# ãƒãƒ¼ã‚¸å…ƒ
from_emoji = ["ğŸ˜‚", "ğŸ’•", "â˜ºï¸", "ğŸ˜ƒ", "ğŸ˜†", "ğŸ˜", "ğŸ¥²", "ğŸ‘", "âœ¨", "â˜€ï¸", "ğŸ˜…", "ğŸ¥º"]
# ãƒãƒ¼ã‚¸å…ˆ
to_emoji = ["ğŸ¤£", "ğŸ¥°", "ğŸ˜Š", "ğŸ˜Š", "ğŸ˜Š", "ğŸ˜Š", "ğŸ˜­", "ğŸ˜Š", "ğŸ˜Š", "ğŸ˜Š", "ğŸ¤£", "ğŸ˜­"]
# ãƒ©ãƒ™ãƒ«ãƒãƒƒãƒ”ãƒ³ã‚°
label2id = {"ğŸ˜­": 0, "ğŸ¤£": 1, "ğŸ‰": 2, "ğŸ¥°": 3, "ğŸ˜‡": 4, "ğŸ’¦": 5, "ğŸ˜Š": 6, "ğŸ¤”": 7}

"""
# ãƒãƒ¼ã‚¸å…ƒ
from_emoji = ["ğŸ˜‚", "ğŸ’•", "â˜ºï¸", "ğŸ˜ƒ", "ğŸ˜†", "ğŸ˜", "ğŸ¥²", "ğŸ‘", "âœ¨", "â˜€ï¸", "ğŸ˜…", "ğŸ¥º", "ğŸ˜‡", "ğŸ’¦"]
# ãƒãƒ¼ã‚¸å…ˆ
to_emoji = ["ğŸ¤£", "ğŸ¥°", "ğŸ˜Š", "ğŸ˜Š", "ğŸ˜Š", "ğŸ˜Š", "ğŸ˜­", "ğŸ˜Š", "ğŸ˜Š", "ğŸ˜Š", "ğŸ¤£", "ğŸ˜­", "ğŸ˜­", "ğŸ˜­"]
# ãƒ©ãƒ™ãƒ«ãƒãƒƒãƒ”ãƒ³ã‚°
label2id = {"ğŸ˜­": 0, "ğŸ¤£": 1, "ğŸ‰": 2, "ğŸ¥°": 3, "ğŸ˜Š": 4, "ğŸ¤”": 5}
"""
############################################################################################################

# ãƒ©ãƒ™ãƒ«
label = [e for e in label2id]
# ãƒ©ãƒ™ãƒ«æ•°
num_labels = len(label)


# json ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ pandas ã«èª­ã¿è¾¼ã‚€
def load_json_to_pandas(json_path):

    data = pd.read_json(json_path)

    # çµµæ–‡å­—ãƒãƒ¼ã‚¸
    if from_emoji:
        for index, row in data.iterrows():
            if row["label"] in from_emoji:
                data.at[index, "label"] = to_emoji[from_emoji.index(row["label"])]

    return data


# pandas ã‹ã‚‰ Dataset ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆåŒ–ã™ã‚‹
def pandas_to_Dataset(dataset):

    dataset = Dataset.from_pandas(dataset)

    return dataset


# DataFrame å‹ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ãƒ©ãƒ™ãƒ«ã‚’æ•´æ•°ã«å¤‰ãˆã‚‹
def label_to_int(dataset):

    for index, row in dataset.iterrows():
        dataset.at[index, "label"] = label2id[row["label"]]

    return dataset


# ãƒˆãƒ¼ã‚¯ãƒ³åŒ–
def tokenize(data):

    tokenizer = AutoTokenizer.from_pretrained(model_name)

    return tokenizer(data["text"], padding="max_length", truncation=True, max_length=128)


# è©•ä¾¡
def compute_metric(eval_pred):

    metric = evaluate.load("accuracy")

    logits, labels = eval_pred
    predictions = np.argmax(logits, axis=-1)
    
    print(classification_report(y_true=labels, y_pred=predictions, target_names=label, zero_division=0.0))
    
    return metric.compute(predictions=predictions, references=labels)


# ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚µãƒ¼ãƒã®ãŸã‚ã®ãƒ¢ãƒ‡ãƒ«ã®èª­ã¿è¾¼ã¿
def model_init():
    return AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=num_labels)


############################################################################################################
# ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚µãƒ¼ãƒã‚¹ãƒšãƒ¼ã‚¹
def optuna_hp_space(trial):
    return {
        "learning_rate": trial.suggest_float("learning_rate", 1e-6, 1e-4, log=True),
        "per_device_train_batch_size": trial.suggest_categorical("per_device_train_batch_size", [8, 16, 32]),
    }
############################################################################################################


def main():

    ############################################################################################################
    # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿è¨­å®š
    # è¨“ç·´ãƒ‡ãƒ¼ã‚¿ãƒ‘ã‚¹
    train_data_path = "./dataset/train_cls/R8.json"
    # ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã®ãƒ‘ã‚¹
    test_data_path = "./dataset/dev/V8h.json"
    # è¨“ç·´ãƒ‡ãƒ¼ã‚¿æ•°
    num_of_train_data = 313770
    # ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿æ•°
    num_of_test_data = 360
    # çµæœãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹
    result_path = "./log_optuna"
    # ã‚¨ãƒãƒƒã‚¯æ•°
    num_epoch = 2
    # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚µãƒ¼ãƒå›æ•°
    n_trials = 5
    ############################################################################################################

    # ç”Ÿãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿ï¼ˆçµµæ–‡å­—ãƒãƒ¼ã‚¸å«ã‚€ï¼‰
    train_data = load_json_to_pandas(train_data_path)
    test_data = load_json_to_pandas(test_data_path)

    # ãƒ©ãƒ™ãƒ«ã‚’æ•´æ•°ã«å¤‰ãˆã‚‹
    train_data = label_to_int(train_data)
    test_data = label_to_int(test_data)
    
    # Dataset ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã«å¤‰ãˆã‚‹
    train_data = pandas_to_Dataset(train_data)
    test_data = pandas_to_Dataset(test_data)
    #print(train_data[1])

    # ãƒˆãƒ¼ã‚¯ãƒ³åŒ–
    train_data = train_data.map(tokenize, batched=True)
    test_data = test_data.map(tokenize, batched=True)
    #print(train_data[0])

    # ã‚ˆã‚Šå°ã•ã„ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ä½œæˆã™ã‚‹
    train_data = train_data.shuffle(seed=42).select(range(num_of_train_data))
    test_data = test_data.shuffle(seed=42).select(range(num_of_test_data))
    #print(len(train_data), len(test_data))
    #print(train_data[0])
    #print(test_data[0])

    # training args ã®ä½œæˆ
    training_args = TrainingArguments(
        output_dir=result_path,
        evaluation_strategy="epoch",
        num_train_epochs=num_epoch,
        logging_strategy="steps",
        logging_steps=0.1,
        report_to="none"
    )

    # è¨“ç·´ãƒ‡ãƒ¼ã‚¿
    trainer = Trainer(
        model_init=model_init,
        args=training_args,
        train_dataset=train_data,
        eval_dataset=test_data,
        compute_metrics=compute_metric
    )
    best_trial = trainer.hyperparameter_search(
        hp_space=optuna_hp_space,
        n_trials=n_trials,
        direction="maximize",
        backend="optuna"
    )
    print(best_trial)

if __name__ == "__main__":
    main()