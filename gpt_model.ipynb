{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "çµµæ–‡å­—ã‚’ãƒãƒ¼ã‚¸ã™ã‚‹ã‹ã©ã†ã‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################################################################\n",
    "# ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿è¨­å®š\n",
    "# çµµæ–‡å­—ã‚’ãƒãƒ¼ã‚¸ã™ã‚‹ã‹ã©ã†ã‹\n",
    "emoji_merge = True\n",
    "# ãƒãƒ¼ã‚¸å…ƒ\n",
    "from_emoji = [\"ğŸ˜‚\", \"ğŸ’•\", \"â˜ºï¸\", \"ğŸ˜ƒ\", \"ğŸ˜†\", \"ğŸ˜\", \"ğŸ¥²\", \"ğŸ‘\", \"âœ¨\", \"â˜€ï¸\", \"ğŸ˜…\", \"ğŸ¥º\"]\n",
    "# ãƒãƒ¼ã‚¸å…ˆ\n",
    "to_emoji = [\"ğŸ¤£\", \"ğŸ¥°\", \"ğŸ˜Š\", \"ğŸ˜Š\", \"ğŸ˜Š\", \"ğŸ˜Š\", \"ğŸ˜­\", \"ğŸ˜Š\", \"ğŸ˜Š\", \"ğŸ˜Š\", \"ğŸ¤£\", \"ğŸ˜­\"]\n",
    "############################################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ»ãƒ†ã‚¹ãƒˆç”¨ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ä½œæˆ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# ãƒ†ã‚¹ãƒˆã®æ™‚ã®ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã®å–å¾—\n",
    "def get_test_data(dataset, num_per_label, completely_random, num_of_completely_random, create_json=False):\n",
    "\n",
    "    # completely random ãƒ‡ãƒ¼ã‚¿ã®å–ã‚Šå‡ºã—\n",
    "    if completely_random:\n",
    "        pass\n",
    "    # å„ãƒ©ãƒ™ãƒ«ã«ã¤ãã€ä½•å€‹ã®ãƒ‡ãƒ¼ã‚¿ã‚’å–ã‚Šå‡ºã™\n",
    "    else:\n",
    "        print(f\"ãƒ‡ãƒ¼ã‚¿ã®æŠ½å‡ºæ–¹æ³•ï¼šå„ãƒ©ãƒ™ãƒ«ã”ã¨ã«ãƒ©ãƒ³ãƒ€ãƒ ã« {num_per_label} ä»¶ã®ãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡ºã™ã‚‹\")\n",
    "        # ãƒ©ãƒ™ãƒ«ã®å–å¾—\n",
    "        emojis = []\n",
    "        for d in dataset:\n",
    "            if d[\"label\"] not in emojis:\n",
    "                emojis.append(d[\"label\"])\n",
    "        #print(len(emojis))\n",
    "        #print(emojis)\n",
    "\n",
    "        # ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ä½œæˆ\n",
    "        test_dataset = []\n",
    "        for i in range(0, len(emojis), 1):\n",
    "            test_dataset_temp = [d for d in dataset if d[\"label\"] == emojis[i]]\n",
    "            test_dataset_temp = random.sample(test_dataset_temp, num_per_label)\n",
    "            for d in test_dataset_temp:\n",
    "                test_dataset.append(d)\n",
    "        #print(len(test_dataset))\n",
    "        #print(test_dataset)\n",
    "        print(f\"ãƒ‡ãƒ¼ã‚¿ã¯å…¨éƒ¨ã§ {num_per_label}ï¼ˆå„ãƒ©ãƒ™ãƒ«ã”ã¨ã®ãƒ‡ãƒ¼ã‚¿æ•°ï¼‰ * {len(emojis)}ï¼ˆãƒ©ãƒ™ãƒ«æ•°ï¼‰ = {len(test_dataset)} ä»¶\")\n",
    "\n",
    "        # json ãƒ•ã‚¡ã‚¤ãƒ«ã®ä½œæˆ\n",
    "        if create_json:\n",
    "            json_path = f\"./dataset/test/test_{len(test_dataset)}_dataset.json\"\n",
    "            with open(json_path, 'w', encoding='utf-8') as f:\n",
    "                json.dump(test_dataset, f, indent=4, ensure_ascii=False)\n",
    "\n",
    "        return test_dataset\n",
    "\n",
    "\n",
    "def get_ft_dataset(num_per_label, ft_random=True):\n",
    "    ############################################################################################################\n",
    "    # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿è¨­å®š\n",
    "    # json ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹\n",
    "    json_path = \"./dataset/top20_one_emoji_dataset.json\"\n",
    "    # æ—¢å­˜ã®è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ãƒ‘ã‚¹\n",
    "    training_path = \"./dataset/train_gpt/R8.json\"\n",
    "    ############################################################################################################\n",
    "    \n",
    "    # json ãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿\n",
    "    with open(json_path, 'r') as f:\n",
    "        dataset = json.load(f)\n",
    "    #print(len(dataset))\n",
    "\n",
    "    # ãƒ‡ãƒ¼ã‚¿ã‚’ãƒ©ãƒ³ãƒ€ãƒ ã«å–ã‚Šå‡ºã™ã€ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã®ä½œæˆã¨ä¸€ç·’\n",
    "    if ft_random:\n",
    "        ft_dataset = get_test_data(dataset=dataset,\n",
    "                                   num_per_label=num_per_label,\n",
    "                                   completely_random=False,\n",
    "                                   num_of_completely_random=False\n",
    "                                   )\n",
    "        #print(len(ft_dataset))\n",
    "        #print(ft_dataset[0])\n",
    "\n",
    "        ft_dataset = [\n",
    "            {\"messages\": [{\"role\": \"user\", \"content\": f\"{d['text']}ã¨ã„ã†ãƒ„ã‚¤ãƒ¼ãƒˆã«æœ€ã‚‚ç›¸å¿œã—ã„çµµæ–‡å­—ã¯ã©ã‚Œï¼Ÿ\\né¸æŠè‚¢ï¼šğŸ˜…ã€â˜€ï¸ã€â˜ºï¸ã€ğŸ˜†ã€ğŸ˜‚ã€ğŸ˜­ã€âœ¨ã€ğŸ¤£ã€ğŸ¥ºã€ğŸ‰ã€ğŸ¥°ã€ğŸ¥²ã€ğŸ˜ƒã€ğŸ’•ã€ğŸ’¦ã€ğŸ˜‡ã€ğŸ˜Šã€ğŸ‘ã€ğŸ˜ã€ğŸ¤”\"},\n",
    "                          {\"role\": \"assistant\", \"content\": d[\"label\"]}]}\n",
    "            for d in ft_dataset\n",
    "        ]\n",
    "        #print(ft_dataset)\n",
    "\n",
    "        ft_dataset = pd.DataFrame(ft_dataset)\n",
    "        ft_dataset.to_json(\"./dataset/train_gpt/dataset_temp.jsonl\", force_ascii=False, lines=True, orient=\"records\")\n",
    "    # æ—¢å­˜ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’å–å¾—ã™ã‚‹\n",
    "    else:\n",
    "        # æ—¢å­˜ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’èª­ã¿è¾¼ã‚€\n",
    "        with open(training_path, 'r') as f:\n",
    "            ft_dataset = [json.loads(d) for d in f.readlines()]\n",
    "        #print(len(ft_dataset))\n",
    "        #print(ft_dataset[0])\n",
    "\n",
    "        ft_dataset = [\n",
    "            {\"messages\": [\n",
    "                {\"role\": \"user\", \"content\": d['messages'][0]['content']},\n",
    "                {\"role\": \"assistant\", \"content\": d['messages'][1]['content']}\n",
    "            ]}\n",
    "            for d in ft_dataset\n",
    "        ]\n",
    "        #print(len(ft_dataset))\n",
    "        #print(ft_dataset[0])\n",
    "\n",
    "        ft_dataset = pd.DataFrame(ft_dataset)\n",
    "        ft_dataset.to_json(\"./dataset/train_gpt/dataset_temp.jsonl\", force_ascii=False, lines=True, orient=\"records\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "æ¨è«–"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "import emoji\n",
    "import openai\n",
    "from openai import OpenAI\n",
    "\n",
    "\n",
    "def main():\n",
    "\n",
    "    ############################################################################################################\n",
    "    # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿è¨­å®š\n",
    "    # id\n",
    "    result_id = \"GPT35_1\"\n",
    "    # json ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹\n",
    "    json_path = \"./dataset/top20_one_emoji_dataset.json\"\n",
    "    # ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ä½œæˆã™ã‚‹ã‹ã©ã†ã‹\n",
    "    create_test_dataset = False\n",
    "    # æ—¢å­˜ã®ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã‚€ç”¨\n",
    "    test_path = \"./dataset/test/T8h.json\"\n",
    "    # ä½¿ã†ãƒ¢ãƒ‡ãƒ«\n",
    "    model = \"gpt-3.5-turbo-1106\"\n",
    "    # temperatureï¼ˆå‡ºåŠ›ã®ãƒ©ãƒ³ãƒ€ãƒ æ€§ï¼‰\n",
    "    temperature = 0\n",
    "    # ãƒ†ã‚¹ãƒˆã ã‘ã®æ™‚ã®ãƒ†ã‚¹ãƒˆã¯ completely random ã‹ã©ã†ã‹ã€False å„ãƒ©ãƒ™ãƒ«ã«ã¤ãã€ä½•å€‹ã®ãƒ‡ãƒ¼ã‚¿ã‚’å–ã‚Šå‡ºã™ã‹\n",
    "    completely_random = False\n",
    "    # completely random ã®æ™‚ã®å–ã‚Šå‡ºã™ãƒ‡ãƒ¼ã‚¿ã®æ•°\n",
    "    num_of_completely_random = 200\n",
    "    # ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ»ãƒ†ã‚¹ãƒˆã ã‘ã®æ™‚ã®å„ãƒ©ãƒ™ãƒ«ã«ã¤ãã€ä½•å€‹ã®ãƒ‡ãƒ¼ã‚¿ã‚’å–ã‚Šå‡ºã™ã‹\n",
    "    num_per_label = 20\n",
    "    # json ãƒ•ã‚¡ã‚¤ãƒ«ã«å‡ºåŠ›ã‹ã©ã†ã‹\n",
    "    create_json = True\n",
    "    ############################################################################################################\n",
    "\n",
    "    # api key ã‚’ãƒ­ãƒ¼ãƒ‰ã™ã‚‹\n",
    "    client = OpenAI()\n",
    "\n",
    "    # json ãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿\n",
    "    with open(json_path, 'r') as f:\n",
    "        topn_one_emoji_dataset = json.load(f)\n",
    "    #print(len(topn_one_emoji_dataset))\n",
    "    #print(topn_one_emoji_dataset[2])\n",
    "\n",
    "    # ãƒ†ã‚¹ãƒˆ\n",
    "    if create_test_dataset:\n",
    "        test_dataset = get_test_data(\n",
    "            dataset=topn_one_emoji_dataset,\n",
    "            num_per_label=num_per_label,\n",
    "            completely_random=completely_random,\n",
    "            num_of_completely_random=num_of_completely_random,\n",
    "            create_json=create_json\n",
    "            )\n",
    "    else:\n",
    "        with open(test_path, 'r') as f:\n",
    "            test_dataset = json.load(f)\n",
    "\n",
    "    # çµµæ–‡å­—ã‚’ãƒãƒ¼ã‚¸ã™ã‚‹\n",
    "    if emoji_merge:\n",
    "        test_dataset_temp = []\n",
    "        for d in test_dataset:\n",
    "            if d[\"label\"] in from_emoji:\n",
    "                d[\"label\"] = to_emoji[from_emoji.index(d[\"label\"])]\n",
    "                test_dataset_temp.append(d)\n",
    "            else:\n",
    "                test_dataset_temp.append(d)\n",
    "        test_dataset = test_dataset_temp\n",
    "\n",
    "    output = []\n",
    "    correct_output = []\n",
    "    wrong_output = []\n",
    "    for d in test_dataset:\n",
    "\n",
    "        # GPT ã®å…¥åŠ›å½¢å¼ã«å¤‰æ›ã™ã‚‹\n",
    "        messages = [\n",
    "            {\"role\": \"user\", \"content\": f\"{d['text']}ã¨ã„ã†ãƒ„ã‚¤ãƒ¼ãƒˆã«æœ€ã‚‚ç›¸å¿œã—ã„çµµæ–‡å­—ã‚’é¸æŠè‚¢ã®ä¸­ã‹ã‚‰ä¸€ã¤é¸ã‚“ã§ãã ã•ã„ã€‚\\\n",
    "ãªãŠã€é¸ã‚“ã ä¸€ã¤ã®çµµæ–‡å­—ã ã‘å›ç­”ã—ã¦ãã ã•ã„ã€‚\\\n",
    "\\nãŸã ã—ã€ğŸ‰ã¯ã€ŒãŠã‚ã§ã¨ã†ã€ã®ã‚ˆã†ãªãŠç¥ã„ã®è¨€è‘‰ã®å¾Œã«ã—ã‹ä»˜ã‘ãªã„ã€\\\n",
    "ğŸ¥°ã¯ã€Œå¥½ãã€ã€ã€Œå¹¸ã›ã€ã€ã€Œç´ æ•µã€ã‚’è¡¨ç¾ã™ã‚‹ã€\\\n",
    "ğŸ˜‡ã¯ã€Œçµ‚ã‚ã£ãŸã€ã€ã€Œã‚‚ã†ãƒ€ãƒ¡ã€ã€ã€Œå¤©ä½¿ã€ã«é–¢é€£ã—ã¦ä½¿ã†ã€\\\n",
    "ğŸ’¦ã¯ã€Œã‚¹ãƒˆãƒ¬ã‚¹ã€ã€ã€Œç·Šå¼µã€ã€ã€Œæ±—ã€ã€ã€Œæš‘ã•ã€ã«é–¢é€£ã—ã¦ä½¿ã†\\\n",
    "ã¨ã„ã†æ¡ä»¶ãŒã‚ã‚‹ã€‚\\\n",
    "\\né¸æŠè‚¢ï¼šğŸ˜Šã€ğŸ¤£ã€ğŸ˜­ã€ğŸ‰ã€ğŸ¥°ã€ğŸ˜‡ã€ğŸ’¦ã€ğŸ¤”\"}\n",
    "        ]\n",
    "\n",
    "        # ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’ç”Ÿæˆã™ã‚‹\n",
    "        try:\n",
    "            time.sleep(5)\n",
    "            response = client.chat.completions.create(\n",
    "                model=model,\n",
    "                messages=messages,\n",
    "                temperature=temperature\n",
    "                )\n",
    "        except openai.APITimeoutError:\n",
    "            time.sleep(120)\n",
    "            response = client.chat.completions.create(\n",
    "                model=model,\n",
    "                messages=messages,\n",
    "                temperature=temperature\n",
    "                )\n",
    "\n",
    "        # ãƒ¢ãƒ‡ãƒ«ãŒç”Ÿæˆã—ãŸçµµæ–‡å­—ã¯ä¸€ã¤ã ã‘ã‹ã©ã†ã‹ã‚’ãƒã‚§ãƒƒã‚¯\n",
    "        # wrong\n",
    "        if emoji.emoji_count(response.choices[0].message.content, unique=True) != 1:\n",
    "            output_temp = {\n",
    "                \"ãƒ¢ãƒ‡ãƒ«\": response.model,\n",
    "                \"temperature\": temperature,\n",
    "                \"å…¥åŠ›\": {\"role\": messages[0]['role'], \"content\": messages[0]['content']},\n",
    "                \"å‡ºåŠ›\": {\"role\": response.choices[0].message.role, \"content\": response.choices[0].message.content},\n",
    "                \"ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒˆãƒ¼ã‚¯ãƒ³æ•°\": response.usage.prompt_tokens,\n",
    "                \"å‡ºåŠ›ãƒˆãƒ¼ã‚¯ãƒ³æ•°\": response.usage.completion_tokens,\n",
    "                \"ç·ãƒˆãƒ¼ã‚¯ãƒ³æ•°\": response.usage.total_tokens,\n",
    "                \"äºˆæ¸¬\": emoji.distinct_emoji_list(response.choices[0].message.content),\n",
    "                \"æ­£è§£\": d[\"label\"],\n",
    "                \"æ­£èª¤\": \"false\"\n",
    "                }\n",
    "            output.append(output_temp)\n",
    "            wrong_output.append(output_temp)\n",
    "        # correct\n",
    "        else:\n",
    "            # æ­£è§£\n",
    "            if emoji.distinct_emoji_list(response.choices[0].message.content)[0] == d[\"label\"]:\n",
    "                output_temp = {\n",
    "                    \"ãƒ¢ãƒ‡ãƒ«\": response.model,\n",
    "                    \"temperature\": temperature,\n",
    "                    \"å…¥åŠ›\": {\"role\": messages[0]['role'], \"content\": messages[0]['content']},\n",
    "                    \"å‡ºåŠ›\": {\"role\": response.choices[0].message.role, \"content\": response.choices[0].message.content},\n",
    "                    \"ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒˆãƒ¼ã‚¯ãƒ³æ•°\": response.usage.prompt_tokens,\n",
    "                    \"å‡ºåŠ›ãƒˆãƒ¼ã‚¯ãƒ³æ•°\": response.usage.completion_tokens,\n",
    "                    \"ç·ãƒˆãƒ¼ã‚¯ãƒ³æ•°\": response.usage.total_tokens,\n",
    "                    \"äºˆæ¸¬\": emoji.distinct_emoji_list(response.choices[0].message.content)[0],\n",
    "                    \"æ­£è§£\": d[\"label\"],\n",
    "                    \"æ­£èª¤\": \"true\"\n",
    "                    }\n",
    "                output.append(output_temp)\n",
    "                correct_output.append(output_temp)\n",
    "            # ä¸æ­£è§£\n",
    "            else:\n",
    "                output_temp = {\n",
    "                    \"ãƒ¢ãƒ‡ãƒ«\": response.model,\n",
    "                    \"temperature\": temperature,\n",
    "                    \"å…¥åŠ›\": {\"role\": messages[0]['role'], \"content\": messages[0]['content']},\n",
    "                    \"å‡ºåŠ›\": {\"role\": response.choices[0].message.role, \"content\": response.choices[0].message.content},\n",
    "                    \"ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒˆãƒ¼ã‚¯ãƒ³æ•°\": response.usage.prompt_tokens,\n",
    "                    \"å‡ºåŠ›ãƒˆãƒ¼ã‚¯ãƒ³æ•°\": response.usage.completion_tokens,\n",
    "                    \"ç·ãƒˆãƒ¼ã‚¯ãƒ³æ•°\": response.usage.total_tokens,\n",
    "                    \"äºˆæ¸¬\": emoji.distinct_emoji_list(response.choices[0].message.content)[0],\n",
    "                    \"æ­£è§£\": d[\"label\"],\n",
    "                    \"æ­£èª¤\": \"false\"\n",
    "                    }\n",
    "                output.append(output_temp)\n",
    "                correct_output.append(output_temp)\n",
    "    print(f\"çµµæ–‡å­—ãŒä¸€ã¤ã ã‘ã®å‡ºåŠ›ã®æ•°ï¼š {len(correct_output)}\")\n",
    "    print(f\"çµµæ–‡å­—ãŒä¸€ã¤ã˜ã‚ƒãªã„ã®å‡ºåŠ›ã®æ•°ï¼š {len(wrong_output)}\")\n",
    "\n",
    "    # json ãƒ•ã‚¡ã‚¤ãƒ«ã®å‡ºåŠ›\n",
    "    # å…¨éƒ¨\n",
    "    json_path = f\"./results/{result_id}/{model.replace('personal::', '').replace('-', '_')}_output.json\"\n",
    "    with open(json_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(output, f, indent=4, ensure_ascii=False)\n",
    "\n",
    "    # çµµæ–‡å­—ãŒä¸€ã¤ã ã‘ã®å‡ºåŠ›\n",
    "    json_path = f\"./results/{result_id}/{model.replace('personal::', '').replace('-', '_')}_correct_output.json\"\n",
    "    with open(json_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(correct_output, f, indent=4, ensure_ascii=False)\n",
    "\n",
    "    # çµµæ–‡å­—ãŒä¸€ã¤ã˜ã‚ƒãªã„å‡ºåŠ›\n",
    "    json_path = f\"./results/{result_id}/{model.replace('personal::', '').replace('-', '_')}_wrong_output.json\"\n",
    "    with open(json_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(wrong_output, f, indent=4, ensure_ascii=False)\n",
    "    \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ãƒ©ãƒ™ãƒ«ã®å–å¾—ãƒ»ãƒãƒƒãƒ”ãƒ³ã‚°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "\n",
    "# ãƒ©ãƒ™ãƒ«ã®å–å¾—\n",
    "def get_label(json_path):\n",
    "    with open(json_path, 'r') as f:\n",
    "        dataset = json.load(f)\n",
    "\n",
    "    # ãƒ©ãƒ™ãƒ«ã®å–å¾—ï¼ˆå‡ºç¾å›æ•°ã«é–¢ä¿‚ãªã„ï¼‰\n",
    "    label = []\n",
    "    for d in dataset:\n",
    "        if d[\"æ­£è§£\"] not in label:\n",
    "            label.append(d[\"æ­£è§£\"])\n",
    "   \n",
    "    return label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "æ¨è«–ã€ç²¾åº¦è¨ˆç®—"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report, ConfusionMatrixDisplay\n",
    "\n",
    "\n",
    "def main():\n",
    "\n",
    "    ############################################################################################################\n",
    "    # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿è¨­å®š\n",
    "    # id\n",
    "    result_id = \"GPT35_1\"\n",
    "    # ãƒ¢ãƒ‡ãƒ«\n",
    "    model = \"gpt-3.5-turbo-1106\"\n",
    "    # json ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹\n",
    "    json_path = \"./results/GPT35_1/gpt_3.5_turbo_1106_output.json\"\n",
    "    ############################################################################################################\n",
    "\n",
    "    # ãƒ©ãƒ™ãƒ«ã®å–å¾—\n",
    "    label = get_label(json_path=json_path)\n",
    "    #print(len(label))\n",
    "    print('ã€'.join(label))\n",
    "    \n",
    "    # json ãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿\n",
    "    json_path = f\"./results/{result_id}/{model.replace('personal::', '').replace('-', '_')}_output.json\"\n",
    "    with open(json_path, 'r') as f:\n",
    "        output = json.load(f)\n",
    "    #print(len(output))\n",
    "    json_path = f\"/results/{result_id}/{model.replace('personal::', '').replace('-', '_')}_correct_output.json\"\n",
    "    with open(json_path, 'r') as f:\n",
    "        correct_output = json.load(f)\n",
    "    #print(len(correct_output))\n",
    "\n",
    "    # ç”¨æ„ã—ãŸãƒ©ãƒ™ãƒ«ä»¥å¤–ã®çµµæ–‡å­—ãŒäºˆæ¸¬ã•ã‚ŒãŸæ•°\n",
    "    num_of_not_in_label = 0\n",
    "    for d in correct_output:\n",
    "        if d[\"äºˆæ¸¬\"] not in label:\n",
    "            num_of_not_in_label = num_of_not_in_label + 1\n",
    "    print(f\"ç”¨æ„ã—ãŸãƒ©ãƒ™ãƒ«ä»¥å¤–ã®çµµæ–‡å­—ãŒäºˆæ¸¬ã•ã‚ŒãŸæ•°: {num_of_not_in_label}\")\n",
    "\n",
    "    # Acc è¨ˆç®—\n",
    "    num_of_true = len([d[\"æ­£èª¤\"] for d in output if d[\"æ­£èª¤\"] == \"true\"])\n",
    "    print(f\"æ­£è§£æ•°ï¼š {num_of_true}\")\n",
    "    print(f\"Accï¼š {(num_of_true / len(output)) * 100}\")\n",
    "\n",
    "    # P R F1 è¨ˆç®—\n",
    "    label_gold = np.array([d[\"æ­£è§£\"] for d in correct_output])\n",
    "    label_pred = np.array([d[\"äºˆæ¸¬\"] for d in correct_output])\n",
    "    print(classification_report(y_true=label_gold, y_pred=label_pred, labels=label, zero_division=0.0))\n",
    "\n",
    "    ConfusionMatrixDisplay.from_predictions(y_true=label_gold, y_pred=label_pred, labels=label)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "emoji_prediction",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
